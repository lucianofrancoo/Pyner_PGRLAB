#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pyner_search v0.1 - Generador de t√©rminos de b√∫squeda para NCBI SRA
=================================================================
Prop√≥sito: Crear t√©rminos de b√∫squeda booleanos optimizados para NCBI SRA
Uso: python3 Pyner_search_v0.1.py <palabras_clave>

Ejemplo de uso:
    python3 Pyner_search_v0.1.py arabidopsis drought
    python3 Pyner_search_v0.1.py solanum lycopersicum nitrogen

El script:
1. Recibe palabras clave del usuario como argumentos de l√≠nea de comandos
2. Env√≠a esas palabras a un LLM local que:
   - Entiende la sintaxis de b√∫squeda de NCBI SRA
   - Genera sin√≥nimos biol√≥gicos relevantes
   - Construye una consulta booleana √≥ptima
3. Devuelve una frase natural y una consulta para usar en NCBI

Output: JSON con dos campos:
- natural_query: descripci√≥n legible de la b√∫squeda
- esearch_query: t√©rmino booleano para copiar en Pyner_v0.2.py o NCBI directamente
"""

import sys          # Para leer argumentos de l√≠nea de comandos
import json         # Para parsear JSON del LLM
import ollama       # Para usar modelo local

# ============================================
# 1Ô∏è‚É£ VALIDAR ENTRADA Y PROCESAR KEYWORDS
# ============================================

# Verificar que el usuario proporcion√≥ palabras clave
if len(sys.argv) < 2:
    print(" Uso: python3 Pyner_search_v0.1.py <keywords separados por espacio>")
    print(" Ejemplo: python3 Pyner_search_v0.1.py arabidopsis drought")
    sys.exit(1)

# Obtener todas las palabras clave proporcionadas
keywords = sys.argv[1:]
print("\n Generando t√©rmino de b√∫squeda a partir de keywords:")
print("   ", ", ".join(keywords))
print("--------------------------------------------------")

# ============================================
# 2Ô∏è‚É£ INICIALIZAR CLIENTE DE OLLAMA
# ============================================

# Cliente para llamar al modelo local (debe estar corriendo localmente)
client = ollama.Client()

# ============================================
# 3Ô∏è‚É£ CONSTRUIR PROMPT PARA EL LLM
# ============================================
# El prompt es muy importante: instrucciones claras = mejores resultados

prompt = f"""
Eres un asistente experto en bioinform√°tica especializado en el uso de NCBI SRA (Sequence Read Archive).

Tu tarea es generar un t√©rmino de b√∫squeda booleano v√°lido para NCBI SRA usando E-utilities.

IMPORTANTE - RESTRICCIONES DE SINTAXIS:
- No uses campos como [substance], [mesh], ni otros espec√≠ficos de PubMed.
- Usa solo campos que existen en SRA:
  [Organism], [All Fields], [Title], [Strategy], [Selection], [Platform], [Source], [Study]
- Si una palabra no encaja en un campo espec√≠fico, usa [All Fields].
- Devuelve la b√∫squeda en formato booleano con AND y OR seg√∫n corresponda.
- No incluyas explicaciones, solo el JSON.

Las palabras clave objetivo son:
{', '.join(keywords)}

Devuelve SOLO un JSON con este formato exacto:
{{
  "natural_query": "frase natural que describe la b√∫squeda",
  "esearch_query": "consulta compatible con NCBI SRA"
}}

Considera incluir al menos 3 sin√≥nimos relevantes y t√©rminos relacionados para maximizar la recuperaci√≥n de datos.
Si no existen sin√≥nimos relevantes, no los fuerces.
"""

# ============================================
# 4Ô∏è‚É£ LLAMAR AL MODELO LOCAL
# ============================================

print("\n Llamando al LLM (qwen2.5:14b - local)...\n")

try:
    # Llamar al modelo con baja temperatura para respuestas m√°s consistentes
    response = client.generate(
        model='qwen2.5:14b',
        prompt=prompt,
        options={'temperature': 0.2}  # 0.2 = determin√≠stico pero flexible
    )
    # Extraer la respuesta del diccionario
    respuesta_llm = response['response'].strip()
except Exception as e:
    print(f" Error al llamar al modelo: {e}")
    print(" Aseg√∫rate de que ollama est√° corriendo: ollama serve")
    sys.exit(1)

# ============================================
# 5Ô∏è‚É£ PARSEAR LA RESPUESTA JSON
# ============================================

print(" Respuesta cruda del modelo:\n")
print(respuesta_llm)
print("\n--------------------------------------------------")

# Intentar extraer JSON de la respuesta
try:
    data = json.loads(respuesta_llm)
    natural_query = data.get("natural_query", "N/A")
    esearch_query = data.get("esearch_query", "N/A")
except json.JSONDecodeError:
    # Si el LLM no devuelve JSON v√°lido, usar la respuesta como est√°
    print(" Advertencia: El modelo no devolvi√≥ JSON v√°lido")
    natural_query = respuesta_llm
    esearch_query = "No disponible"

# ============================================
# 6Ô∏è‚É£ MOSTRAR RESULTADOS
# ============================================

print(" T√©rmino de b√∫squeda generado:\n")
print(f" Natural query : {natural_query}")
print(f" ESearch query : {esearch_query}\n")

print("--------------------------------------------------")
print(" üìã Instrucciones:")
print(f"    1. Copia el valor de 'esearch_query'")
print(f"    2. P√©galo en la variable 'search_term' de Pyner_v0.2.py")
print(f"    3. Ejecuta: python3 Pyner_v0.2.py")
print("--------------------------------------------------")
